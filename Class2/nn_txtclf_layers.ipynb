{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "# Package imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nimport matplotlib\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom pprint import pprint\n\ncategories \u003d [\u0027alt.atheism\u0027, \u0027talk.religion.misc\u0027, \u0027comp.graphics\u0027, \u0027sci.space\u0027]\n\nnewsgroups_train \u003d fetch_20newsgroups(subset\u003d\u0027train\u0027,  categories\u003dcategories)\nnewsgroups_test \u003d fetch_20newsgroups(subset\u003d\u0027test\u0027,  categories\u003dcategories)"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "outputs": [],
      "source": "num_train \u003d len(newsgroups_train.data)\nnum_test  \u003d len(newsgroups_test.data)\n\n# 提取tfidf特征 TODO\nvectorizer \u003d TfidfVectorizer(max_features\u003d20)\n\n# 对训练和测试数据一起提取特征\nX \u003d vectorizer.fit_transform( newsgroups_train.data + newsgroups_test.data )\n\n# 分离出训练数据和测试数据\nX_train \u003d X[0:num_train, :]\nX_test \u003d X[num_train:num_train+num_test,:]\n\nY_train \u003d newsgroups_train.target\nY_test \u003d newsgroups_test.target\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "(2034, 20) (2034,)\n(1353, 20) (1353,)\n2034\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(X_train.shape, Y_train.shape)\nprint(X_test.shape, Y_test.shape)\nprint(len(X_train.toarray()))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [],
      "source": "# Helper function to plot a decision boundary.\n# If you don\u0027t fully understand this function don\u0027t worry, it just generates the contour plot below.\ndef plot_decision_boundary(pred_func):\n    # Set min and max values and give it some padding\n    x_min, x_max \u003d X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max \u003d X[:, 1].min() - .5, X[:, 1].max() + .5\n    h \u003d 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy \u003d np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole gid\n    Z \u003d pred_func(np.c_[xx.ravel(), yy.ravel()])\n    Z \u003d Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap\u003dplt.cm.Spectral)\n    plt.scatter(X[:, 0], X[:, 1], c\u003dy, cmap\u003dplt.cm.Spectral)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "outputs": [],
      "source": "def calculate_loss(model, X, y):\n    W1, b1, W2, b2, W3, b3, W4, b4 \u003d model[\u0027W1\u0027], model[\u0027b1\u0027], model[\u0027W2\u0027], model[\u0027b2\u0027], model[\u0027W3\u0027], model[\u0027b3\u0027], model[\u0027W4\u0027], model[\u0027b4\u0027]\n    #正向传播，计算预测值\n    z1 \u003d X.dot(W1) + b1\n    a1 \u003d np.maximum(0, z1)\n    z2 \u003d a1.dot(W2) + b2\n    a2 \u003d np.tanh(z2)\n    z3 \u003d a2.dot(W3) + b3\n    a3 \u003d 1./(1 + np.exp(-z3))\n    z4 \u003d a3.dot(W4) + b4\n    exp_scores \u003d np.exp(z4)\n    probs \u003d exp_scores / np.sum(exp_scores, axis\u003d1, keepdims\u003dTrue)\n    # 计算损失\n    corect_logprobs \u003d -np.log(probs[range(num_examples), y])\n    data_loss \u003d np.sum(corect_logprobs)\n    #在损失上加上正则项（可选）\n    data_loss +\u003d reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)) + np.sum(np.square(W4)))\n    return 1./num_examples * data_loss",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "outputs": [],
      "source": "# Helper function to predict an output (0 or 1)\ndef predict(model, X):\n    W1, b1, W2, b2, W3, b3, W4, b4 \u003d model[\u0027W1\u0027], model[\u0027b1\u0027], model[\u0027W2\u0027], model[\u0027b2\u0027], model[\u0027W3\u0027], model[\u0027b3\u0027], model[\u0027W4\u0027], model[\u0027b4\u0027]\n    # 正向传播\n    z1 \u003d X.dot(W1) + b1\n    a1 \u003d np.maximum(0, z1)\n    z2 \u003d a1.dot(W2) + b2\n    a2 \u003d np.tanh(z2)\n    z3 \u003d a2.dot(W3) + b3\n    a3 \u003d 1./(1 + np.exp(-z3))\n    z4 \u003d a3.dot(W4) + b4\n    exp_scores \u003d np.exp(z4)\n    probs \u003d exp_scores / np.sum(exp_scores, axis\u003d1, keepdims\u003dTrue)\n    return np.argmax(probs, axis\u003d1)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "outputs": [],
      "source": "# 这个函数为神经网络学习参数并且返回模型\n# - nn_hdim: 隐藏层的节点数\n# - num_passes: 通过训练集进行梯度下降的次数\n# - print_loss: 如果是True, 那么每1000次迭代就打印一次损失值\ndef build_model(X, y, nn_hdim, epsilon, reg_lambda, num_passes\u003d20000,  print_loss\u003dFalse):\n    \n    # 用随机数初始化参数\n    np.random.seed(0)\n    W1 \u003d np.random.randn(input_dim, nn_hdim[0]) / np.sqrt(input_dim)\n    b1 \u003d np.zeros((1, nn_hdim[0]))\n    W2 \u003d np.random.randn(nn_hdim[0],nn_hdim[1]) / np.sqrt(nn_hdim[0])\n    b2 \u003d np.zeros((1, nn_hdim[1]))\n    W3 \u003d np.random.randn(nn_hdim[1],nn_hdim[2]) / np.sqrt(nn_hdim[1])\n    b3 \u003d np.zeros((1, nn_hdim[2]))\n    W4 \u003d np.random.randn(nn_hdim[2], np.shape(categories)[0]) / np.sqrt(nn_hdim[2])\n    b4 \u003d np.zeros((1, np.shape(categories)[0]))\n\n    model \u003d {}\n \n    # 梯度下降\n    for i in range(0, num_passes):\n        # 正向传播，计算判断出的结果\n        z1 \u003d X.dot(W1) + b1\n        a1 \u003d np.maximum(0, z1)\n        z2 \u003d a1.dot(W2) + b2\n        a2 \u003d np.tanh(z2)\n        z3 \u003d a2.dot(W3) + b3\n        a3 \u003d 1./(1 + np.exp(-z3))\n        z4 \u003d a3.dot(W4) + b4\n        exp_scores \u003d np.exp(z4)\n        probs \u003d exp_scores / np.sum(exp_scores, axis\u003d1, keepdims\u003dTrue)\n        \n        # 反向传播，对参数进行优化\n        # TODO num_examples ?\u003d nn_input_dim\n        delta4 \u003d probs\n        delta4[range(num_examples), y] -\u003d 1\n        dW4 \u003d (a3.T).dot(delta4)\n        db4 \u003d np.sum(delta4, axis\u003d0, keepdims\u003dTrue)\n        delta3 \u003d delta4.dot(W4.T) * a3 * (1 - a3)\n        dW3 \u003d (a2.T).dot(delta3)\n        db3 \u003d np.sum(delta3, axis\u003d0, keepdims\u003dTrue)\n        delta2 \u003d delta3.dot(W3.T) * (1 - np.power(a2, 2))\n        dW2 \u003d (a1.T).dot(delta2)\n        db2 \u003d np.sum(delta2, axis\u003d0)\n        delta1 \u003d delta2.dot(W2.T)\n        delta1[z1 \u003c\u003d 0] \u003d 0\n        dW1 \u003d (X.T).dot(delta1)\n        db1 \u003d np.sum(delta1, axis\u003d0)\n        \n        # 添加正则项\n        dW4 +\u003d reg_lambda * W4\n        dW3 +\u003d reg_lambda * W3\n        dW2 +\u003d reg_lambda * W2\n        dW1 +\u003d reg_lambda * W1\n        \n        # 梯度下降更新参数\n        W1 +\u003d -epsilon * dW1\n        b1 +\u003d -epsilon * db1\n        W2 +\u003d -epsilon * dW2\n        b2 +\u003d -epsilon * db2\n        W3 +\u003d -epsilon * dW3\n        b3 +\u003d -epsilon * db3\n        W4 +\u003d -epsilon * dW4\n        b4 +\u003d -epsilon * db4\n        \n        # 为模型分配新参数\n        model \u003d { \u0027W1\u0027: W1, \u0027b1\u0027: b1, \u0027W2\u0027: W2, \u0027b2\u0027: b2, \u0027W3\u0027: W3, \u0027b3\u0027: b3, \u0027W4\u0027: W4, \u0027b4\u0027: b4 }\n        \n        # 选择性打印loss\n        if print_loss and i % 1000 \u003d\u003d 0:\n            print(\"Loss after iteration\", i, calculate_loss(model, X, y))\n    return model\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "outputs": [],
      "source": "num_examples, input_dim \u003d X_train.shape # 样本数量，输入层维度（target种类数）\nepsilon \u003d 0.001 # 梯度下降学习率\nreg_lambda \u003d 0.01 # 正则化强度\nepochs \u003d 5000 # 梯度下降的次数:1个epoch表示过了1遍训练集中的所有样本\nnn_hdims \u003d [4, 8, 4]",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Loss after iteration 0 1.3717489680050339\n",
            "Loss after iteration 1000 1.165192184947891\n",
            "Loss after iteration 2000 1.1515157872576371\n",
            "Loss after iteration 3000 1.1427322748471165\n",
            "Loss after iteration 4000 1.1312670307174053\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "model \u003d build_model(X_train, Y_train, nn_hdims, epsilon, reg_lambda, epochs, print_loss\u003dTrue)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Accuracy 0.447154 \u003d 605 / 1353\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "n_correct \u003d 0\nn_test \u003d X_test.shape[0]\nfor n in range(n_test):\n    x \u003d X_test[n,:]\n    yp \u003d predict(model, x)\n    if yp \u003d\u003d Y_test[n]:\n        n_correct +\u003d 1.0\n\nprint(\u0027Accuracy %f \u003d %d / %d\u0027%(n_correct/n_test, int(n_correct), n_test) )\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}